{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2465e221",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Test imports after fix\n",
    "print(\"üß™ Testing imports...\")\n",
    "\n",
    "try:\n",
    "    print(\"Testing basic imports...\")\n",
    "    import torch\n",
    "    print(\"‚úì torch imported successfully\")\n",
    "    \n",
    "    import numpy as np\n",
    "    print(f\"‚úì numpy imported successfully (version: {np.__version__})\")\n",
    "    \n",
    "    import pandas as pd\n",
    "    print(f\"‚úì pandas imported successfully (version: {pd.__version__})\")\n",
    "    \n",
    "    print(\"\\nTesting lerobot imports...\")\n",
    "    from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "    print(\"‚úì LeRobotDataset imported successfully\")\n",
    "    \n",
    "    from lerobot.common.datasets.utils import dataset_to_policy_features\n",
    "    print(\"‚úì dataset_to_policy_features imported successfully\")\n",
    "    \n",
    "    from lerobot.common.policies.act.configuration_act import ACTConfig\n",
    "    print(\"‚úì ACTConfig imported successfully\")\n",
    "    \n",
    "    from lerobot.common.policies.act.modeling_act import ACTPolicy\n",
    "    print(\"‚úì ACTPolicy imported successfully\")\n",
    "    \n",
    "    from lerobot.configs.types import FeatureType\n",
    "    print(\"‚úì FeatureType imported successfully\")\n",
    "    \n",
    "    from lerobot.common.datasets.factory import resolve_delta_timestamps\n",
    "    print(\"‚úì resolve_delta_timestamps imported successfully\")\n",
    "    \n",
    "    import torchvision\n",
    "    print(\"‚úì torchvision imported successfully\")\n",
    "    \n",
    "    print(\"\\nüéâ All imports successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee2e090",
   "metadata": {},
   "source": [
    "# Train Action-Chunking-Transformer (ACT) on your Dataset\n",
    "\n",
    "Train the ACT model on your custom dataset. In this example, we set chunk_size as 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "from lerobot.common.datasets.utils import dataset_to_policy_features\n",
    "from lerobot.common.policies.act.configuration_act import ACTConfig\n",
    "from lerobot.common.policies.act.modeling_act import ACTPolicy\n",
    "from lerobot.configs.types import FeatureType\n",
    "from lerobot.common.datasets.factory import resolve_delta_timestamps\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Number of offline training steps (we'll only do offline training for this example.)\n",
    "# Adjust as you prefer. 5000 steps are needed to get something worth evaluating.\n",
    "training_steps = 3000\n",
    "log_freq = 100\n",
    "\n",
    "# Check for potential PIL issues and provide helpful guidance\n",
    "try:\n",
    "    import PIL\n",
    "    pil_version = PIL.__version__\n",
    "    print(f\"üì∏ PIL/Pillow version: {pil_version}\")\n",
    "    \n",
    "    # Check if we have a known problematic version\n",
    "    if \"10.\" in pil_version or \"11.\" in pil_version:\n",
    "        print(\"‚ö†Ô∏è  Note: You might encounter PIL AttributeError issues with this version.\")\n",
    "        print(\"üí° If you see PIL errors, the notebook has fallbacks to handle them gracefully.\")\n",
    "        print(\"   Training will still work, just some visualizations might be skipped.\")\n",
    "    else:\n",
    "        print(\"‚úÖ PIL version looks good!\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå PIL/Pillow not found - this might cause issues\")\n",
    "    print(\"üí° Consider installing: pip install pillow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a791f",
   "metadata": {},
   "source": [
    "## Policy Configuration and Initialize\n",
    "\n",
    "chunk_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a634a9b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üìä Dataset Inspection\n",
    "\n",
    "Let's start by understanding our dataset. We'll check the total number of frames (steps) available for training and inspect the statistics (mean and std) for the state and action spaces. These stats were computed when the dataset was created and are crucial for normalizing the data before feeding it to the policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa145a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Let's create a simple dataset first to inspect it (no transforms yet)\n",
    "simple_dataset = LeRobotDataset(\"omy_pnp\", root='./demo_data')\n",
    "\n",
    "print(f\"üóÇÔ∏è  The dataset contains {len(simple_dataset)} steps of experience.\")\n",
    "print(f\"üéØ  Number of episodes: {len(simple_dataset.meta.episodes)}\")\n",
    "\n",
    "print(\"\\nüìà Dataset statistics used for normalization:\")\n",
    "for key, stats in simple_dataset.meta.stats.items():\n",
    "    print(f\"  {key}:\")\n",
    "    if 'mean' in stats:\n",
    "        print(f\"    mean: {stats['mean']}\")\n",
    "        print(f\"    std:  {stats['std']}\")\n",
    "    if 'min' in stats:\n",
    "        print(f\"    min:  {stats['min']}\")\n",
    "        print(f\"    max:  {stats['max']}\")\n",
    "    print()\n",
    "\n",
    "# Let's look at one sample to understand the data structure\n",
    "sample = simple_dataset[0]\n",
    "print(\"üîç Sample data structure:\")\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value)} - {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa2397",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üß† Model Architecture Analysis\n",
    "\n",
    "Next, let's instantiate our policy and analyze its complexity. The number of trainable parameters tells us about the model's capacity to learn (and overfit!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc1fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When starting from scratch (i.e. not from a pretrained policy), we need to specify 2 things before\n",
    "# creating the policy:\n",
    "#   - input/output shapes: to properly size the policy\n",
    "#   - dataset stats: for normalization and denormalization of input/outputs\n",
    "dataset_metadata = LeRobotDatasetMetadata(\"omy_pnp\", root='./demo_data')\n",
    "features = dataset_to_policy_features(dataset_metadata.features)\n",
    "output_features = {key: ft for key, ft in features.items() if ft.type is FeatureType.ACTION}\n",
    "input_features = {key: ft for key, ft in features.items() if key not in output_features}\n",
    "input_features.pop(\"observation.wrist_image\")\n",
    "# Policies are initialized with a configuration class, in this case `DiffusionConfig`. For this example,\n",
    "# we'll just use the defaults and so no arguments other than input/output features need to be passed.\n",
    "cfg = ACTConfig(input_features=input_features, output_features=output_features, chunk_size= 10, n_action_steps=10)\n",
    "# This allows us to construct the data with action chunking\n",
    "delta_timestamps = resolve_delta_timestamps(cfg, dataset_metadata)\n",
    "# We can now instantiate our policy with this config and the dataset stats.\n",
    "policy = ACTPolicy(cfg, dataset_stats=dataset_metadata.stats)\n",
    "policy.train()\n",
    "policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd6928",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# üßÆ Let's analyze our policy architecture and count parameters\n",
    "total_params = sum(p.numel() for p in policy.parameters() if p.requires_grad)\n",
    "total_params_million = total_params / 1_000_000\n",
    "\n",
    "print(f\"üß† The ACT policy has {total_params:,} trainable parameters ({total_params_million:.2f}M)\")\n",
    "\n",
    "print(f\"\\nüîß Policy Configuration:\")\n",
    "print(f\"  Input features: {list(input_features.keys())}\")\n",
    "print(f\"  Output features: {list(output_features.keys())}\")\n",
    "print(f\"  Chunk size: {cfg.chunk_size} (predicts {cfg.chunk_size} actions at once)\")\n",
    "print(f\"  Action steps: {cfg.n_action_steps} (uses {cfg.n_action_steps} actions from the chunk)\")\n",
    "\n",
    "print(f\"\\nüèóÔ∏è  Model Architecture:\")\n",
    "print(f\"   Hidden dimension: {cfg.dim_model}\")\n",
    "print(f\"   Encoder layers: {cfg.n_encoder_layers}\")\n",
    "print(f\"   Decoder layers: {cfg.n_decoder_layers}\")\n",
    "print(f\"   Attention heads: {cfg.n_heads}\")\n",
    "\n",
    "# Let's break down parameter count by component\n",
    "print(f\"\\nüìä Parameter breakdown:\")\n",
    "for name, module in policy.named_children():\n",
    "    if hasattr(module, 'parameters'):\n",
    "        param_count = sum(p.numel() for p in module.parameters())\n",
    "        percentage = (param_count / total_params) * 100\n",
    "        print(f\"   {name}: {param_count:,} params ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dac077",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e334c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üé® Data Augmentation Visualization\n",
    "\n",
    "Let's understand what our data augmentation is doing to the images. The current transform adds Gaussian noise to make the model more robust to variations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb69000",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define our augmentation transform first\n",
    "class AddGaussianNoise(object):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to a tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., std=0.01):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        # Adds noise: tensor remains a tensor.\n",
    "        noise = torch.randn(tensor.size()) * self.std + self.mean\n",
    "        return tensor + noise\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\"\n",
    "\n",
    "# Create a transformation pipeline that converts a PIL image to a tensor, then adds noise.\n",
    "transform = transforms.Compose([\n",
    "    AddGaussianNoise(mean=0., std=0.02),\n",
    "    transforms.Lambda(lambda x: x.clamp(0, 1))\n",
    "])\n",
    "\n",
    "# To avoid PIL issues, let's work with tensor data directly from our existing dataset\n",
    "# The simple_dataset was already loaded successfully above\n",
    "\n",
    "# Get a sample without transforms (from our working simple_dataset)\n",
    "print(\"üîç Getting sample data for augmentation visualization...\")\n",
    "try:\n",
    "    sample_data = simple_dataset[0]\n",
    "    original_image = sample_data['observation.image']\n",
    "    print(f\"‚úÖ Successfully loaded sample image with shape: {original_image.shape}\")\n",
    "    \n",
    "    # Apply our transform to see the difference\n",
    "    augmented_image = transform(original_image)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(original_image.permute(1, 2, 0))\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Augmented image \n",
    "    axes[1].imshow(augmented_image.permute(1, 2, 0))\n",
    "    axes[1].set_title(\"Augmented Image (with Gaussian Noise)\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üîç Image shape: {original_image.shape}\")\n",
    "    print(f\"üìä Original image stats:\")\n",
    "    print(f\"   Mean: {original_image.mean():.3f}\")\n",
    "    print(f\"   Std:  {original_image.std():.3f}\")\n",
    "    print(f\"   Min:  {original_image.min():.3f}\")\n",
    "    print(f\"   Max:  {original_image.max():.3f}\")\n",
    "    \n",
    "    print(f\"üìä Augmented image stats:\")\n",
    "    print(f\"   Mean: {augmented_image.mean():.3f}\")\n",
    "    print(f\"   Std:  {augmented_image.std():.3f}\")\n",
    "    print(f\"   Min:  {augmented_image.min():.3f}\")\n",
    "    print(f\"   Max:  {augmented_image.max():.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading image data: {e}\")\n",
    "    print(\"‚ö†Ô∏è  This might be a PIL version compatibility issue.\")\n",
    "    print(\"üí° The training will still work, but visualization is skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b165c2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Transform definition moved to the previous cell to fix ordering issues\n",
    "print(\"‚úÖ Transform already defined above - ready for dataset creation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4219dfa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# We can then instantiate the dataset with these delta_timestamps configuration.\n",
    "print(\"üîÑ Creating dataset with transforms...\")\n",
    "try:\n",
    "    dataset = LeRobotDataset(\"omy_pnp\", delta_timestamps=delta_timestamps, root='./demo_data', image_transforms=transform)\n",
    "    print(\"‚úÖ Dataset created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating dataset with transforms: {e}\")\n",
    "    print(\"üîÑ Falling back to dataset without image transforms...\")\n",
    "    # Fall back to dataset without image transforms if there are PIL issues\n",
    "    dataset = LeRobotDataset(\"omy_pnp\", delta_timestamps=delta_timestamps, root='./demo_data')\n",
    "    print(\"‚úÖ Dataset created without image transforms (training will still work)\")\n",
    "\n",
    "# Then we create our optimizer and dataloader for offline training.\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=4,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=device.type != \"cpu\",\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print(f\"üì¶ DataLoader created with batch size {dataloader.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e73680",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üîç Batch Structure Analysis\n",
    "\n",
    "Let's inspect exactly what data structure the model receives during training. Understanding the batch format is crucial for debugging and understanding how the model processes the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202bf59",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Let's inspect one batch to understand the data structure\n",
    "print(\"üîç Attempting to load a batch for inspection...\")\n",
    "try:\n",
    "    batch = next(iter(dataloader))\n",
    "    print(\"‚úÖ Batch loaded successfully!\")\n",
    "    \n",
    "    print(\"üéØ Batch Structure (what the model receives):\")\n",
    "    print(f\"   Batch size: {batch['observation.image'].shape[0]}\")\n",
    "    print()\n",
    "    \n",
    "    for key, value in batch.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"üì¶ {key}:\")\n",
    "            print(f\"   Shape: {value.shape}\")\n",
    "            print(f\"   Dtype: {value.dtype}\")\n",
    "            print(f\"   Device: {value.device}\")\n",
    "            if 'image' in key:\n",
    "                print(f\"   Value range: [{value.min():.3f}, {value.max():.3f}]\")\n",
    "            elif 'action' in key or 'state' in key:\n",
    "                print(f\"   Value range: [{value.min():.3f}, {value.max():.3f}]\")\n",
    "                print(f\"   Mean: {value.mean():.3f}, Std: {value.std():.3f}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"üì¶ {key}: {type(value)} - {value if isinstance(value, (str, int, float)) else '...'}\")\n",
    "            print()\n",
    "    \n",
    "    # Let's understand the action chunking\n",
    "    print(\"üéØ Understanding Action Chunking:\")\n",
    "    action_tensor = batch['action']\n",
    "    print(f\"   Action tensor shape: {action_tensor.shape}\")\n",
    "    print(f\"   Interpretation: [batch_size={action_tensor.shape[0]}, chunk_size={action_tensor.shape[1]}, action_dim={action_tensor.shape[2]}]\")\n",
    "    print(f\"   This means each input predicts {action_tensor.shape[1]} future actions\")\n",
    "    print(f\"   Each action has {action_tensor.shape[2]} dimensions (6 joint angles + 1 gripper)\")\n",
    "    \n",
    "    # Show one example action sequence\n",
    "    print(f\"\\nüìä Example action sequence from batch (first item):\")\n",
    "    example_actions = action_tensor[0]  # First item in batch\n",
    "    for i, action in enumerate(example_actions):\n",
    "        print(f\"   Step {i}: {[f'{x:.3f}' for x in action.tolist()]}\")\n",
    "        if i >= 3:  # Only show first few steps\n",
    "            print(f\"   ... (and {len(example_actions)-4} more steps)\")\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading batch: {e}\")\n",
    "    print(\"‚ö†Ô∏è  This is likely due to PIL version compatibility issues.\")\n",
    "    print(\"üí° The training loop will handle this gracefully.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "019a5a8d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üöÄ Enhanced Training Loop with Analytics\n",
    "\n",
    "Let's train our policy with detailed monitoring. I'll add timing information, loss tracking, and estimated time remaining. \n",
    "\n",
    "**Important Note on Loss Behavior:**\n",
    "- When you re-run this cell, the loss will jump back up because we re-initialize the model weights\n",
    "- With only one demonstration episode, the model will quickly overfit (loss drops very low)\n",
    "- This doesn't mean it will generalize well - it just memorized the one trajectory!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb76650",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "The trained checkpoint will be saved in './ckpt/act_y' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73e5d1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# Enhanced training loop with timing and analytics\n",
    "step = 0\n",
    "done = False\n",
    "start_time = time.time()\n",
    "loss_history = deque(maxlen=100)  # Keep track of recent losses\n",
    "best_loss = float('inf')\n",
    "\n",
    "print(f\"üöÄ Starting training for {training_steps} steps...\")\n",
    "print(f\"üìä Logging every {log_freq} steps\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "while not done:\n",
    "    for batch in dataloader:\n",
    "        # Move batch to device\n",
    "        inp_batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        loss, _ = policy.forward(inp_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Track loss\n",
    "        current_loss = loss.item()\n",
    "        loss_history.append(current_loss)\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "\n",
    "        # Logging and progress tracking\n",
    "        if step % log_freq == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            steps_per_second = (step + 1) / elapsed_time if elapsed_time > 0 else 0\n",
    "            remaining_steps = training_steps - step\n",
    "            eta_seconds = remaining_steps / steps_per_second if steps_per_second > 0 else 0\n",
    "            eta_minutes = eta_seconds / 60\n",
    "            \n",
    "            # Calculate average loss over recent steps\n",
    "            avg_recent_loss = sum(loss_history) / len(loss_history) if loss_history else current_loss\n",
    "            \n",
    "            print(f\"üî• Step {step:4d}/{training_steps} | \"\n",
    "                  f\"Loss: {current_loss:.3f} | \"\n",
    "                  f\"Avg: {avg_recent_loss:.3f} | \"\n",
    "                  f\"Best: {best_loss:.3f} | \"\n",
    "                  f\"Speed: {steps_per_second:.1f} steps/s | \"\n",
    "                  f\"ETA: {eta_minutes:.1f}m\")\n",
    "            \n",
    "            # Check for potential overfitting warning\n",
    "            if step > 500 and len(loss_history) == 100:\n",
    "                recent_improvement = loss_history[0] - loss_history[-1]\n",
    "                if recent_improvement < 0.001:  # Very little improvement\n",
    "                    print(\"‚ö†Ô∏è  Warning: Loss plateaued - possible overfitting with single demo!\")\n",
    "\n",
    "        step += 1\n",
    "        if step >= training_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úÖ Training completed!\")\n",
    "print(f\"‚è±Ô∏è  Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"üéØ Final loss: {current_loss:.3f}\")\n",
    "print(f\"üèÜ Best loss: {best_loss:.3f}\")\n",
    "print(f\"‚ö° Average speed: {training_steps/total_time:.1f} steps/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d307994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the policy to disk.\n",
    "policy.save_pretrained('./ckpt/act_y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab65e74c",
   "metadata": {},
   "source": [
    "## Test Inference\n",
    "\n",
    "To evaluate the policy on the dataset, you can calculate the error between ground-truth actions from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EpisodeSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset: LeRobotDataset, episode_index: int):\n",
    "        from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "        to_idx = dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "        self.frame_ids = range(from_idx, to_idx)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.frame_ids)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.frame_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb7760",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ Setting up inference evaluation...\")\n",
    "policy.eval()\n",
    "actions = []\n",
    "gt_actions = []\n",
    "images = []\n",
    "episode_index = 0\n",
    "\n",
    "try:\n",
    "    episode_sampler = EpisodeSampler(dataset, episode_index)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        num_workers=4,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        pin_memory=device.type != \"cpu\",\n",
    "        sampler=episode_sampler,\n",
    "    )\n",
    "    \n",
    "    print(f\"üî¨ Running inference on episode {episode_index}...\")\n",
    "    print(f\"üìè Episode length: {len(episode_sampler)} steps\")\n",
    "    \n",
    "    policy.reset()\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        inp_batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "        action = policy.select_action(inp_batch)\n",
    "        actions.append(action)\n",
    "        gt_actions.append(inp_batch[\"action\"][:,0,:])\n",
    "        images.append(inp_batch[\"observation.image\"])\n",
    "        \n",
    "        # Show progress for long episodes\n",
    "        if i % 50 == 0:\n",
    "            print(f\"   Processed {i}/{len(episode_sampler)} steps...\")\n",
    "    \n",
    "    actions = torch.cat(actions, dim=0)\n",
    "    gt_actions = torch.cat(gt_actions, dim=0)\n",
    "    \n",
    "    # Calculate detailed error metrics\n",
    "    mean_abs_error = torch.mean(torch.abs(actions - gt_actions)).item()\n",
    "    max_abs_error = torch.max(torch.abs(actions - gt_actions)).item()\n",
    "    mse = torch.mean((actions - gt_actions)**2).item()\n",
    "    \n",
    "    print(f\"\\nüìä Evaluation Results:\")\n",
    "    print(f\"   Mean Absolute Error: {mean_abs_error:.4f}\")\n",
    "    print(f\"   Max Absolute Error:  {max_abs_error:.4f}\")\n",
    "    print(f\"   Mean Squared Error:  {mse:.4f}\")\n",
    "    \n",
    "    # Per-joint analysis\n",
    "    joint_names = ['Joint 1', 'Joint 2', 'Joint 3', 'Joint 4', 'Joint 5', 'Joint 6', 'Gripper']\n",
    "    print(f\"\\nüîß Per-joint errors:\")\n",
    "    for i, joint_name in enumerate(joint_names):\n",
    "        joint_error = torch.mean(torch.abs(actions[:, i] - gt_actions[:, i])).item()\n",
    "        print(f\"   {joint_name}: {joint_error:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüí° Interpretation:\")\n",
    "    if mean_abs_error < 0.01:\n",
    "        print(\"   ‚úÖ Very low error - model has memorized the demonstration well\")\n",
    "        print(\"   ‚ö†Ô∏è  But this doesn't guarantee generalization to new situations!\")\n",
    "    elif mean_abs_error < 0.1:\n",
    "        print(\"   ‚úÖ Low error - model learned the general trajectory\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  High error - model may need more training or better data\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during inference evaluation: {e}\")\n",
    "    print(\"‚ö†Ô∏è  This is likely due to PIL version compatibility issues.\")\n",
    "    print(\"üí° Your model was still trained successfully! The evaluation step is optional.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7fdd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Visualize Predicted vs Ground Truth Actions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "action_dim = 7\n",
    "joint_names = ['Joint 1', 'Joint 2', 'Joint 3', 'Joint 4', 'Joint 5', 'Joint 6', 'Gripper']\n",
    "\n",
    "# Create subplots for each joint\n",
    "fig, axes = plt.subplots(action_dim, 1, figsize=(15, 3*action_dim))\n",
    "if action_dim == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(action_dim):\n",
    "    axes[i].plot(gt_actions[:, i].cpu().numpy(), label='Ground Truth', linewidth=2, alpha=0.8)\n",
    "    axes[i].plot(actions[:, i].cpu().numpy(), label='Predicted', linewidth=2, alpha=0.8, linestyle='--')\n",
    "    axes[i].set_title(f'{joint_names[i]} - Predicted vs Ground Truth')\n",
    "    axes[i].set_xlabel('Time Step')\n",
    "    axes[i].set_ylabel('Action Value')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate and show error for this joint\n",
    "    joint_error = torch.mean(torch.abs(actions[:, i] - gt_actions[:, i])).item()\n",
    "    axes[i].text(0.02, 0.98, f'MAE: {joint_error:.4f}', \n",
    "                transform=axes[i].transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('üéØ Model Performance: Predicted vs Ground Truth Actions', \n",
    "             fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"üìä Summary Statistics:\")\n",
    "print(f\"   Dataset contains {len(actions)} action steps\")\n",
    "print(f\"   Overall Mean Absolute Error: {mean_abs_error:.4f}\")\n",
    "print(f\"   Best performing joint: {joint_names[torch.argmin(torch.mean(torch.abs(actions - gt_actions), dim=0))]}\")\n",
    "print(f\"   Worst performing joint: {joint_names[torch.argmax(torch.mean(torch.abs(actions - gt_actions), dim=0))]}\")\n",
    "\n",
    "if mean_abs_error < 0.01:\n",
    "    print(f\"\\nüéâ Excellent! The model has learned the demonstration very well.\")\n",
    "    print(f\"   Next steps: Collect more diverse demonstrations for better generalization!\")\n",
    "else:\n",
    "    print(f\"\\nüîß Consider: More training steps, different learning rate, or data quality issues.\")\n",
    "\n",
    "fig, axs = plt.subplots(action_dim, 1, figsize=(10, 10))\n",
    "\n",
    "for i in range(action_dim):\n",
    "    axs[i].plot(actions[:, i].cpu().detach().numpy(), label=\"pred\")\n",
    "    axs[i].plot(gt_actions[:, i].cpu().detach().numpy(), label=\"gt\")\n",
    "    axs[i].legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
